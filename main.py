import aiohttp
import os
from typing import Optional, Dict, Any, List
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api.message_components import Image, Plain, Video, File
from astrbot.api import logger

# æ–‡ä»¶ç±»å‹å¯¹åº”çš„ Emoji æ˜ å°„
EMOJI_MAP = {
    "IMAGE": "ğŸ–¼ï¸",
    "VIDEO": "ğŸ“¹",
    "PDF": "ğŸ“„",
    "ZIP": "ğŸ“¦",
    "BOOK": "ğŸ“š",
    "TEXT": "ğŸ“",
    "DEFAULT": "ğŸ“"
}

@register("astrbot_plugin_meta_collect", "yunus", "å…ƒé‡‡é›†å¹³å°æœç´¢æ’ä»¶", "1.0.0")
class MelonSearchPlugin(Star):
    def __init__(self, context: Context):
        super().__init__(context)
        self.base_url = "http://localhost:8080"
        self._session: Optional[aiohttp.ClientSession] = None

    async def initialize(self):
        """åˆå§‹åŒ–æ—¶åˆ›å»ºæŒä¹…åŒ–çš„ HTTP ä¼šè¯"""
        self._session = aiohttp.ClientSession()

    async def terminate(self):
        """ç»ˆæ­¢æ—¶å…³é—­ä¼šè¯"""
        if self._session:
            await self._session.close()

    # ==========================================================
    # å·¥å…·æ–¹æ³•
    # ==========================================================

    async def _fetch_json(self, url: str, params: Optional[Dict] = None) -> Optional[Any]:
        """ç»Ÿä¸€çš„ HTTP GET è¯·æ±‚æ–¹æ³•ï¼Œè¿”å› JSON æ•°æ®"""
        try:
            async with self._session.get(url, params=params) as resp:
                if resp.status != 200:
                    logger.warning(f"HTTP è¯·æ±‚å¤±è´¥: {url}, çŠ¶æ€ç : {resp.status}")
                    return None
                return await resp.json()
        except Exception as e:
            logger.error(f"HTTP è¯·æ±‚å¼‚å¸¸: {url}, é”™è¯¯: {e}")
            return None

    def _extract_first_item(self, data: Any) -> Optional[Dict]:
        """ä»å“åº”æ•°æ®ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆé¡¹"""
        if isinstance(data, list) and data:
            return data[0]
        elif isinstance(data, dict):
            return data
        return None

    async def _fetch_oss_url(self, oss_id: str) -> Optional[tuple[str, str]]:
        """è·å– OSS æ–‡ä»¶çš„çœŸå® URL å’ŒåŸå§‹æ–‡ä»¶å

        Returns:
            tuple[url, original_name] æˆ– None
        """
        url = f"{self.base_url}/resource/oss/web/listByIds/{oss_id}"
        oss_json = await self._fetch_json(url)

        if not oss_json or oss_json.get("code") != 200:
            return None

        oss_data = oss_json.get("data", [])
        if not oss_data:
            return None

        real_url = oss_data[0].get("url")
        original_name = oss_data[0].get("originalName", "æœªå‘½åæ–‡ä»¶")

        return (real_url, original_name) if real_url else None

    async def _get_all_group_files(self, group_id: int, bot) -> List[Dict]:
        """
        è·å–ç¾¤æ–‡æ¡£ä¸­çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨ï¼ˆé€’å½’è·å–æ‰€æœ‰æ–‡ä»¶å¤¹ï¼‰
        å…¼å®¹ç°æœ‰çš„ç¾¤æ–‡ä»¶è·å–å®ç°

        Returns:
            åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º dictï¼Œè‡³å°‘åŒ…å« 'file_name' å’Œ 'file_id'
        """
        try:
            from .src.file_ops import get_all_files_recursive_core
            all_files = await get_all_files_recursive_core(group_id, bot)
            logger.info(f"ä»ç¾¤ {group_id} è·å–åˆ° {len(all_files)} ä¸ªç¾¤æ–‡æ¡£æ–‡ä»¶")
            return all_files
        except Exception as e:
            logger.warning(f"è·å–ç¾¤æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _find_file_in_group(self, code: str, group_files: List[Dict]) -> Optional[Dict]:
        """
        åœ¨ç¾¤æ–‡æ¡£åˆ—è¡¨ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        æ–‡ä»¶åè§„åˆ™ï¼šæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä¸ code å®Œå…¨ä¸€è‡´

        Args:
            code: è¦æŸ¥æ‰¾çš„ code
            group_files: ç¾¤æ–‡æ¡£æ–‡ä»¶åˆ—è¡¨

        Returns:
            åŒ¹é…çš„æ–‡ä»¶ä¿¡æ¯ dict æˆ– None
        """
        if not group_files:
            return None

        for file_info in group_files:
            file_name = file_info.get('file_name', '')
            # å»é™¤æ‰©å±•å
            base_name, _ = os.path.splitext(file_name)

            # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä¸ code å®Œå…¨åŒ¹é…
            if base_name == code:
                logger.info(f"åœ¨ç¾¤æ–‡æ¡£ä¸­æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {file_name} (code: {code})")
                return file_info

        return None

    def _format_search_result(self, item: Dict) -> str:
        """æ ¼å¼åŒ–å•æ¡æœç´¢ç»“æœ"""
        cid = item.get("code") or item.get("id")
        title = item.get("title", "æ— æ ‡é¢˜")
        file_type = item.get("fileType", "DEFAULT")
        emoji = EMOJI_MAP.get(file_type, EMOJI_MAP["DEFAULT"])
        return f" {emoji}ã€{cid}ã€‘{title}"

    # ==========================================================
    # æŒ‡ä»¤ï¼š/æœç“œ [å…³é”®è¯]
    # ==========================================================

    @filter.command("æœç“œ")
    async def search_melon(self, event: AstrMessageEvent):
        """æœç´¢èµ„æºï¼š/æœç“œ <å…³é”®è¯>"""
        # è§£æå…³é”®è¯
        parts = event.message_str.split(maxsplit=1)
        if len(parts) < 2 or not parts[1].strip():
            yield event.plain_result("âŒ è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š/æœç“œ demo")
            return

        keyword = parts[1].strip()

        # è¯·æ±‚æœç´¢æ¥å£
        url = f"{self.base_url}/media/mediaData/web/list"
        params = {"contentText": keyword, "status": "enable"}

        data = await self._fetch_json(url, params)

        if data is None:
            yield event.plain_result(f"âŒ æœç´¢æ¥å£è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
            return

        if not data:
            yield event.plain_result(f"ğŸ” æœç´¢ã€{keyword}ã€‘ï¼ŒæœªæŸ¥åˆ°ç›¸å…³ä¿¡æ¯")
            return

        # æ„å»ºç»“æœæ¶ˆæ¯
        count = len(data)
        msg_lines = [f"ğŸ” æœç´¢ã€{keyword}ã€‘ï¼Œå…±æŸ¥åˆ° {count} æ¡ä¿¡æ¯ï¼š\n"]
        msg_lines.extend(self._format_search_result(item) for item in data)
        msg_lines.append("\nğŸ’¡ è¾“å…¥ /cid [CODE] è·å–è¯¦æƒ…")
        msg_lines.append("ğŸ”‘ å¦‚éœ€è§£å‹å¯†ç è¯·æŸ¥çœ‹å…¬å‘Š")

        yield event.plain_result("\n".join(msg_lines))

    # ==========================================================
    # æŒ‡ä»¤ï¼š/cid [CODE]
    # ==========================================================

    @filter.command("cid")
    async def query_detail(self, event: AstrMessageEvent):
        """è·å–èµ„æºè¯¦æƒ…ï¼š/cid <CODE>"""
        # è§£æ CODE å‚æ•°
        parts = event.message_str.split(maxsplit=1)
        if len(parts) < 2 or not parts[1].strip():
            yield event.plain_result("âŒ è¯·è¾“å…¥ Codeï¼Œä¾‹å¦‚ï¼š/cid 2001")
            return

        cid_arg = parts[1].strip()
        yield event.plain_result(f"â³ æ­£åœ¨æŸ¥è¯¢ {cid_arg}ï¼Œè¯·ç¨ç­‰...")

        # 1. å°è¯•ä»ç¾¤æ–‡æ¡£è·å–æ–‡ä»¶
        group_id_str = event.get_group_id()
        group_file = None

        if group_id_str and event.bot:
            try:
                group_id = int(group_id_str)
                logger.info(f"æ­£åœ¨æ£€æŸ¥ç¾¤ {group_id} çš„æ–‡æ¡£...")
                group_files = await self._get_all_group_files(group_id, event.bot)
                group_file = self._find_file_in_group(cid_arg, group_files)

                if group_file:
                    logger.info(f"âœ… åœ¨ç¾¤æ–‡æ¡£ä¸­æ‰¾åˆ°æ–‡ä»¶ï¼Œè·³è¿‡ç½‘ç»œè¯·æ±‚")
                    # ç›´æ¥è¿”å›ç¾¤æ–‡æ¡£æ–‡ä»¶
                    chain = await self._build_group_file_chain(group_file, cid_arg)
                    yield event.chain_result(chain)
                    return
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ç¾¤æ–‡æ¡£æ—¶å‡ºé”™: {e}ï¼Œç»§ç»­ä½¿ç”¨ç½‘ç»œæŸ¥è¯¢")

        # 2. ç¾¤æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ï¼ŒæŸ¥è¯¢è¯¦æƒ…æ¥å£
        query_url = f"{self.base_url}/media/mediaData/web/query"
        raw_data = await self._fetch_json(query_url, params={"code": cid_arg})

        if raw_data is None:
            yield event.plain_result("âŒ è¯¦æƒ…æ¥å£è¯·æ±‚å¤±è´¥")
            return

        data = self._extract_first_item(raw_data)
        if not data:
            yield event.plain_result(f"âŒ æœªæ‰¾åˆ° Code ä¸º {cid_arg} çš„èµ„æº")
            return

        # 3. æ„å»ºæ¶ˆæ¯é“¾ï¼ˆç½‘ç»œæ–‡ä»¶ï¼‰
        chain = await self._build_detail_chain(data, cid_arg)
        yield event.chain_result(chain)

    async def _build_detail_chain(self, data: Dict, cid_arg: str) -> List:
        """æ„å»ºè¯¦æƒ…æ¶ˆæ¯é“¾"""
        chain = []

        # åŸºç¡€ä¿¡æ¯
        title = data.get("title", "æ— æ ‡é¢˜")
        cover_url = data.get("coverUrl", "")
        file_type = data.get("fileType", "DEFAULT")
        netdisk_type = data.get("netdiskType", "æœªçŸ¥ç½‘ç›˜")
        netdisk_url = data.get("netdiskUrl", "")

        # æ·»åŠ æ ‡é¢˜
        emoji = EMOJI_MAP.get(file_type, EMOJI_MAP["DEFAULT"])
        chain.append(Plain(f"{emoji} {title}\n"))

        # æ·»åŠ å°é¢
        if cover_url:
            chain.append(Image.fromURL(cover_url))

        # æ ¹æ®æ–‡ä»¶ç±»å‹æ·»åŠ å†…å®¹
        if file_type == "IMAGE":
            await self._add_images(chain, data)
        elif file_type == "VIDEO":
            await self._add_videos(chain, data)
        elif file_type in ["ZIP", "PDF"]:
            await self._add_files(chain, data, cid_arg)
        else:
            chain.append(Plain("\nâš ï¸ æš‚ä¸æ”¯æŒè¯¥æ–‡ä»¶ç±»å‹ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"))

        # æ·»åŠ ç½‘ç›˜ä¿¡æ¯
        if netdisk_url:
            chain.append(Plain(f"\nğŸ“Œ è¯¦æƒ…ï¼šã€{netdisk_type}ã€‘\nğŸ”— {netdisk_url}"))

        return chain

    async def _build_group_file_chain(self, group_file: Dict, code: str) -> List:
        """
        æ„å»ºç¾¤æ–‡æ¡£æ–‡ä»¶çš„æ¶ˆæ¯é“¾

        Args:
            group_file: ç¾¤æ–‡æ¡£æ–‡ä»¶ä¿¡æ¯
            code: æ–‡ä»¶ code
        """
        chain = []

        file_name = group_file.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
        file_id = group_file.get('file_id', '')
        # æ³¨æ„ï¼šä½ çš„å®ç°ä¸­ä½¿ç”¨çš„æ˜¯ 'size' è€Œä¸æ˜¯ 'file_size'
        file_size = group_file.get('size', 0)
        parent_folder = group_file.get('parent_folder_name', 'æ ¹ç›®å½•')

        # æ ¹æ®æ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹
        _, ext = os.path.splitext(file_name)
        ext_upper = ext.upper().lstrip('.')

        # æ˜ å°„æ‰©å±•ååˆ° emoji
        emoji = EMOJI_MAP.get(ext_upper, EMOJI_MAP["DEFAULT"])

        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        size_str = self._format_file_size(file_size)

        # æ·»åŠ æ ‡é¢˜å’Œä¿¡æ¯
        text = (
            f"{emoji} ç¾¤æ–‡ä»¶å†…å·²å­˜åœ¨: {file_name}\n"
            f"ğŸ“‚ æ‰€åœ¨æ–‡ä»¶å¤¹ï¼š{parent_folder}\n"
            f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{size_str}\n"
            f"ğŸ”‘ å¦‚éœ€è§£å‹å¯†ç è¯·æŸ¥çœ‹å…¬å‘Š"
        )

        chain.append(Plain(text))

        logger.info(f"ä»ç¾¤æ–‡æ¡£è¿”å›æ–‡ä»¶: {file_name}, å¤§å°: {size_str}")

        return chain

    def _format_file_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.2f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.2f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.2f} GB"

    async def _add_images(self, chain: List, data: Dict):
        """æ·»åŠ å›¾ç‰‡åˆ°æ¶ˆæ¯é“¾"""
        images_str = data.get("imagesUrl", "")
        if images_str:
            img_urls = [url.strip() for url in images_str.split(",") if url.strip()]
            for url in img_urls:
                chain.append(Image.fromURL(url))

    async def _add_videos(self, chain: List, data: Dict):
        """æ·»åŠ è§†é¢‘åˆ°æ¶ˆæ¯é“¾"""
        video_urls = data.get("videoUrls", "")
        if not video_urls:
            chain.append(Plain("\nâš ï¸ æœªæ‰¾åˆ°è§†é¢‘èµ„æº"))
            return

        oss_ids = [i.strip() for i in video_urls.split(",") if i.strip()]
        for oss_id in oss_ids:
            result = await self._fetch_oss_url(oss_id)
            if result:
                real_url, _ = result
                chain.append(Video.fromURL(real_url))

    async def _add_files(self, chain: List, data: Dict, fallback_name: str):
        """æ·»åŠ æ–‡ä»¶åˆ°æ¶ˆæ¯é“¾"""
        file_urls = data.get("fileUrls", "")
        if not file_urls:
            chain.append(Plain("\nâš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶èµ„æº"))
            return

        oss_ids = [i.strip() for i in file_urls.split(",") if i.strip()]
        for oss_id in oss_ids:
            result = await self._fetch_oss_url(oss_id)
            if result:
                real_url, original_name = result
                chain.append(File(url=real_url, name=original_name or fallback_name))